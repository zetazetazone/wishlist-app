---
phase: 38-url-scraping
plan: 02
type: execute
wave: 2
depends_on: ["38-01"]
files_modified:
  - lib/urlScraper.ts
autonomous: true

must_haves:
  truths:
    - "Client can call scrapeUrl() and receive typed ScrapeResult"
    - "URL normalization removes tracking parameters before scraping"
    - "Client handles Edge Function errors gracefully"
  artifacts:
    - path: "lib/urlScraper.ts"
      provides: "Client service for URL scraping"
      exports: ["scrapeUrl", "normalizeUrl", "isValidUrl"]
  key_links:
    - from: "lib/urlScraper.ts"
      to: "supabase.functions.invoke"
      via: "Edge Function invocation"
      pattern: "supabase\\.functions\\.invoke.*scrape-url"
    - from: "lib/urlScraper.ts"
      to: "types/scraping.types.ts"
      via: "type imports"
      pattern: "import.*ScrapeResult.*scraping\\.types"
---

<objective>
Create the client service layer for URL scraping.

Purpose: Provides typed, ergonomic client API for invoking the scrape-url Edge Function with URL normalization and error handling.

Output: `lib/urlScraper.ts` with `scrapeUrl()` function that UI components can call.
</objective>

<execution_context>
@/home/zetaz/.claude/get-shit-done/workflows/execute-plan.md
@/home/zetaz/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/38-url-scraping/38-RESEARCH.md
@.planning/phases/38-url-scraping/38-01-SUMMARY.md

# Existing patterns
@lib/supabase.ts
@lib/claims.ts
@types/scraping.types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create URL scraper client service</name>
  <files>lib/urlScraper.ts</files>
  <action>
Create `lib/urlScraper.ts` following existing lib/*.ts patterns:

```typescript
/**
 * URL Scraper Library
 * Client service for URL metadata extraction via Edge Function
 */

import { supabase } from './supabase';
import type { ScrapedMetadata, ScrapeResult, ScrapeErrorCode } from '../types/scraping.types';

// Re-export types for convenience
export type { ScrapedMetadata, ScrapeResult, ScrapeErrorCode };

// Common tracking parameters to remove
const TRACKING_PARAMS = [
  'utm_source', 'utm_medium', 'utm_campaign', 'utm_term', 'utm_content',
  'ref', 'tag', 'fbclid', 'gclid', 'msclkid', 'dclid',
  'mc_cid', 'mc_eid', 'zanpid', '_ga', 'sref'
];

/**
 * Validate if string is a valid HTTP/HTTPS URL
 */
export function isValidUrl(url: string): boolean {
  try {
    const parsed = new URL(url);
    return ['http:', 'https:'].includes(parsed.protocol);
  } catch {
    return false;
  }
}

/**
 * Normalize URL by removing tracking params and ensuring HTTPS
 */
export function normalizeUrl(url: string): string {
  try {
    const parsed = new URL(url);

    // Remove tracking parameters
    TRACKING_PARAMS.forEach(param => parsed.searchParams.delete(param));

    // Ensure HTTPS
    if (parsed.protocol === 'http:') {
      parsed.protocol = 'https:';
    }

    return parsed.toString();
  } catch {
    return url;
  }
}

/**
 * Scrape metadata from a product URL
 *
 * @param url - The product URL to scrape
 * @returns ScrapeResult with metadata on success, error details on failure
 *
 * @example
 * const result = await scrapeUrl('https://amazon.com/dp/B09V3KXJPB');
 * if (result.success && result.data) {
 *   console.log(result.data.title, result.data.price);
 * } else {
 *   console.log('Scraping failed:', result.error);
 * }
 */
export async function scrapeUrl(url: string): Promise<ScrapeResult> {
  // Client-side URL validation
  if (!isValidUrl(url)) {
    return {
      success: false,
      error: 'Invalid URL format',
      code: 'INVALID_URL'
    };
  }

  // Normalize URL before sending to Edge Function
  const normalizedUrl = normalizeUrl(url);

  try {
    const { data, error } = await supabase.functions.invoke<ScrapeResult>('scrape-url', {
      body: { url: normalizedUrl }
    });

    if (error) {
      console.error('Edge Function invocation error:', error);
      return {
        success: false,
        error: error.message || 'Failed to invoke scrape function',
        code: 'SCRAPE_FAILED'
      };
    }

    // Edge Function returns ScrapeResult directly
    if (!data) {
      return {
        success: false,
        error: 'No response from scrape function',
        code: 'SCRAPE_FAILED'
      };
    }

    return data;
  } catch (err) {
    console.error('Unexpected error during scrape:', err);
    return {
      success: false,
      error: err instanceof Error ? err.message : 'Unknown error',
      code: 'SCRAPE_FAILED'
    };
  }
}
```

Key implementation notes:
- Follow existing lib/*.ts naming convention (camelCase file name)
- Re-export types for component convenience
- Client-side validation before Edge Function call (saves round trip)
- Normalize URL to remove tracking params
- Handle all error cases gracefully
- JSDoc with example for discoverability
  </action>
  <verify>
1. TypeScript compiles: `npx tsc --noEmit`
2. Imports resolve: Check that `../types/scraping.types` import works
3. Supabase client is available: Check `./supabase` import works
  </verify>
  <done>
lib/urlScraper.ts exists with scrapeUrl(), normalizeUrl(), and isValidUrl() functions, properly typed with ScrapeResult return type.
  </done>
</task>

<task type="auto">
  <name>Task 2: Test client service integration</name>
  <files>lib/urlScraper.ts</files>
  <action>
Create a simple test in the Expo app to verify end-to-end integration:

1. Temporarily add to any existing screen (e.g., at end of My Wishlist screen useEffect):
```typescript
import { scrapeUrl } from '../lib/urlScraper';

// Test scraping (remove after verification)
useEffect(() => {
  const testScrape = async () => {
    console.log('Testing URL scraper...');
    const result = await scrapeUrl('https://www.amazon.com/dp/B09V3KXJPB');
    console.log('Scrape result:', JSON.stringify(result, null, 2));
  };
  testScrape();
}, []);
```

2. Start Edge Function locally: `npx supabase functions serve scrape-url --no-verify-jwt`
3. Run Expo app: `npx expo start`
4. Check console for scrape result

5. After verification, REMOVE the test code from the screen.

Note: This is a smoke test only. The actual UI integration is in Plan 03.
  </action>
  <verify>
1. Console shows "Testing URL scraper..."
2. Console shows scrape result with title and/or imageUrl
3. No TypeScript errors in the component
4. Test code is REMOVED after verification
  </verify>
  <done>
Client service successfully invokes Edge Function and receives typed response. Integration verified and test code removed.
  </done>
</task>

</tasks>

<verification>
1. TypeScript compiles without errors: `npx tsc --noEmit`
2. lib/urlScraper.ts exists with exported functions
3. scrapeUrl() successfully calls Edge Function
4. Error cases return proper ScrapeResult structure
</verification>

<success_criteria>
- [ ] lib/urlScraper.ts exists with scrapeUrl, normalizeUrl, isValidUrl exports
- [ ] Types are properly imported from types/scraping.types
- [ ] scrapeUrl returns ScrapeResult type
- [ ] Client-side URL validation works (invalid URL returns immediately)
- [ ] End-to-end integration with Edge Function verified
</success_criteria>

<output>
After completion, create `.planning/phases/38-url-scraping/38-02-SUMMARY.md`
</output>
