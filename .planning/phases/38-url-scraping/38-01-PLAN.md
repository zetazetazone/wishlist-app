---
phase: 38-url-scraping
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - types/scraping.types.ts
  - supabase/functions/scrape-url/index.ts
autonomous: true

must_haves:
  truths:
    - "Edge Function accepts POST with URL and returns structured metadata"
    - "Extraction works for OG tags, JSON-LD, and HTML fallbacks"
    - "Invalid URLs return 400 with INVALID_URL error code"
    - "Blocked/failed sites return 200 with success:false and SCRAPE_FAILED code"
  artifacts:
    - path: "types/scraping.types.ts"
      provides: "ScrapedMetadata, ScrapeResult, ScrapeErrorCode types"
      exports: ["ScrapedMetadata", "ScrapeResult", "ScrapeErrorCode"]
    - path: "supabase/functions/scrape-url/index.ts"
      provides: "Edge Function for URL metadata extraction"
      contains: "serve(async"
  key_links:
    - from: "supabase/functions/scrape-url/index.ts"
      to: "cheerio"
      via: "npm:cheerio@1.0.0 import"
      pattern: "import.*cheerio.*npm:cheerio"
---

<objective>
Create the URL scraping Edge Function and TypeScript types for metadata extraction.

Purpose: Server-side scraping bypasses CORS restrictions and extracts product metadata (title, image, price, description) from any URL using Open Graph, JSON-LD, and HTML fallback patterns.

Output: Working Edge Function at `scrape-url` that can be invoked via `supabase.functions.invoke()`.
</objective>

<execution_context>
@/home/zetaz/.claude/get-shit-done/workflows/execute-plan.md
@/home/zetaz/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/38-url-scraping/38-RESEARCH.md

# Existing patterns
@supabase/functions/push/index.ts
@types/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create scraping TypeScript types</name>
  <files>types/scraping.types.ts</files>
  <action>
Create `types/scraping.types.ts` with:

```typescript
/**
 * Scraping Types
 * Types for URL metadata extraction
 */

export interface ScrapedMetadata {
  title: string | null;
  description: string | null;
  imageUrl: string | null;
  price: number | null;
  currency: string | null;
  siteName: string | null;
  sourceUrl: string;
}

export type ScrapeErrorCode =
  | 'INVALID_URL'
  | 'SCRAPE_FAILED'
  | 'TIMEOUT'
  | 'BLOCKED';

export interface ScrapeResult {
  success: boolean;
  data?: ScrapedMetadata;
  error?: string;
  code?: ScrapeErrorCode;
}
```

Also add export to `types/index.ts`:
```typescript
export * from './scraping.types';
```
  </action>
  <verify>
Run `npx tsc --noEmit` - no type errors for new types.
Check `types/index.ts` exports scraping types.
  </verify>
  <done>
ScrapedMetadata, ScrapeResult, and ScrapeErrorCode types exist and are exported from types/index.ts.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create scrape-url Edge Function</name>
  <files>supabase/functions/scrape-url/index.ts</files>
  <action>
Create `supabase/functions/scrape-url/index.ts` following the push/index.ts pattern:

1. **Imports** (use existing Deno patterns):
```typescript
import { serve } from 'https://deno.land/std@0.168.0/http/server.ts';
import * as cheerio from "npm:cheerio@1.0.0";
```

2. **CORS headers** (same as push function):
```typescript
const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
};
```

3. **Browser-like fetch headers** (from research):
```typescript
const BROWSER_HEADERS = {
  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
  'Accept-Language': 'en-US,en;q=0.9',
  'Accept-Encoding': 'gzip, deflate, br',
  'Connection': 'keep-alive',
  'Upgrade-Insecure-Requests': '1',
  'Sec-Fetch-Dest': 'document',
  'Sec-Fetch-Mode': 'navigate',
  'Sec-Fetch-Site': 'none',
  'Sec-Fetch-User': '?1',
  'Cache-Control': 'max-age=0',
};
```

4. **Helper functions**:
   - `isValidProductUrl(url: string): boolean` - validates http/https URLs
   - `normalizeUrl(url: string): string` - removes tracking params, ensures https
   - `parsePrice(priceStr: string): number | null` - handles US/EU formats
   - `extractFromJsonLd($: cheerio.CheerioAPI): Partial<ScrapedMetadata>` - parses JSON-LD Product schema
   - `extractMetadata(html: string, url: string): ScrapedMetadata` - main extraction with fallback chain

5. **Extraction fallback chain** (from research):
   - Title: og:title -> twitter:title -> title tag -> h1
   - Description: og:description -> twitter:description -> meta description
   - Image: og:image -> twitter:image -> meta itemprop=image (resolve relative URLs)
   - Price: product:price:amount -> JSON-LD offers.price -> HTML selectors
   - Currency: product:price:currency -> JSON-LD priceCurrency -> default 'USD'
   - Site name: og:site_name -> domain from URL

6. **Main handler**:
   - Handle OPTIONS for CORS
   - Parse { url } from request body
   - Validate URL, return 400 with INVALID_URL if invalid
   - Fetch with 10s timeout using AbortController
   - Extract metadata using cheerio
   - Return { success: true, data: metadata } on success
   - Return { success: false, error, code: 'SCRAPE_FAILED' } on failure (status 200, not 500)

Key implementation notes:
- Use `npm:cheerio@1.0.0` specifier (NOT esm.sh, NOT import map)
- Timeout at 10 seconds (not 150s Edge Function limit)
- Always return 200 for scrape failures (graceful degradation)
- Log errors but don't expose internal details to client
  </action>
  <verify>
1. Run `npx supabase functions serve scrape-url --no-verify-jwt` to start locally
2. Test with curl:
```bash
# Valid URL test
curl -X POST http://localhost:54321/functions/v1/scrape-url \
  -H "Content-Type: application/json" \
  -d '{"url": "https://www.amazon.com/dp/B09V3KXJPB"}'

# Invalid URL test (should return INVALID_URL)
curl -X POST http://localhost:54321/functions/v1/scrape-url \
  -H "Content-Type: application/json" \
  -d '{"url": "not-a-url"}'
```
3. Verify response has { success: true, data: { title, imageUrl, ... } } structure
  </verify>
  <done>
Edge Function scrape-url exists, handles CORS, validates URLs, extracts metadata with fallback chain, and returns structured ScrapeResult.
  </done>
</task>

</tasks>

<verification>
1. TypeScript compiles without errors: `npx tsc --noEmit`
2. Edge Function serves locally: `npx supabase functions serve scrape-url --no-verify-jwt`
3. Valid URL returns metadata with title, imageUrl, siteName at minimum
4. Invalid URL returns { success: false, code: 'INVALID_URL' }
5. Types are exported from types/index.ts
</verification>

<success_criteria>
- [ ] types/scraping.types.ts exists with ScrapedMetadata, ScrapeResult, ScrapeErrorCode
- [ ] supabase/functions/scrape-url/index.ts exists and serves
- [ ] Edge Function extracts og:title, og:image from test URL
- [ ] Edge Function handles invalid URLs gracefully (400 response)
- [ ] Edge Function handles failed scrapes gracefully (200 with success: false)
</success_criteria>

<output>
After completion, create `.planning/phases/38-url-scraping/38-01-SUMMARY.md`
</output>
